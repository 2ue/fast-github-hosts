#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GitHub Hosts è‡ªåŠ¨ç”Ÿæˆå·¥å…· - Ultimateç‰ˆæœ¬
åŠŸèƒ½ï¼šDoHæŸ¥è¯¢ + çº¯TCPæµ‹é€Ÿ + æ™ºèƒ½ç¼“å­˜ + åŸŸååˆ†çº§ + Daemonæ¨¡å¼ + HTTP API + ç»Ÿè®¡æŠ¥å‘Š
ä½œè€…ï¼šåŸºäºProç‰ˆæœ¬ç»ˆæä¼˜åŒ–
ç‰ˆæœ¬ï¼š3.0.0 Ultimate
"""

import dns.resolver
import socket
import concurrent.futures
import time
import json
import argparse
import os
import logging
import sys
from datetime import datetime
from typing import List, Dict, Tuple, Optional
from pathlib import Path

# ==================== ç‰ˆæœ¬ä¿¡æ¯ ====================
VERSION = "3.0.0"
PROGRAM_NAME = "GitHub Hosts Ultimate"

# ==================== æ—¥å¿—é…ç½® ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('github_hosts_ultimate.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==================== é…ç½®åŒº ====================

# åŸŸåé…ç½®æ–‡ä»¶è·¯å¾„
DOMAINS_FILE = 'github_domains.json'

def load_github_domains() -> List[str]:
    """ä»é…ç½®æ–‡ä»¶åŠ è½½GitHubåŸŸååˆ—è¡¨"""
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        domains_path = os.path.join(script_dir, DOMAINS_FILE)

        with open(domains_path, 'r', encoding='utf-8') as f:
            domains = json.load(f)

        return domains
    except FileNotFoundError:
        logger.error(f"æ‰¾ä¸åˆ°åŸŸåé…ç½®æ–‡ä»¶ {DOMAINS_FILE}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        logger.error(f"åŸŸåé…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"åŠ è½½åŸŸåé…ç½®å¤±è´¥: {e}")
        sys.exit(1)

# åŠ è½½æ‰€æœ‰åŸŸå
ALL_DOMAINS = load_github_domains()

# åŸŸååˆ†çº§æ•°é‡é…ç½®(åŸºäºALL_DOMAINSçš„åˆ‡ç‰‡è€Œéç¡¬ç¼–ç åˆ†ç»„)
DOMAIN_COUNT = {
    'core': 30,       # æ ¸å¿ƒåŸŸå:å‰30ä¸ª
    'extended': 70,   # æ‰©å±•åŸŸå:å‰70ä¸ª
    'full': len(ALL_DOMAINS)  # å…¨éƒ¨åŸŸå
}


# DoHæœåŠ¡å™¨
DOH_SERVERS = [
    'https://1.1.1.1/dns-query',
    'https://8.8.8.8/resolve',
    'https://223.5.5.5/resolve',
]

# ä¼ ç»ŸDNSæœåŠ¡å™¨
DNS_SERVERS = ['1.1.1.1', '8.8.8.8', '223.5.5.5', '114.114.114.114']

# æµ‹é€Ÿé…ç½®
TCP_TEST_COUNT = 3
TCP_TIMEOUT = 2
TCP_PORT = 443
MAX_WORKERS = 10
TOP_IP_COUNT = 3

# ç¼“å­˜é…ç½®
CACHE_FILE = '.github_hosts_cache.json'
CACHE_ENABLED = True

# ==================== DoHæŸ¥è¯¢æ¨¡å— ====================

def query_dns_doh(domain: str, doh_server: str) -> List[str]:
    """ä½¿ç”¨DNS-over-HTTPSæŸ¥è¯¢"""
    try:
        import requests
        response = requests.get(
            doh_server,
            params={'name': domain, 'type': 'A'},
            headers={'accept': 'application/dns-json'},
            timeout=3
        )
        if response.status_code == 200:
            data = response.json()
            return [ans['data'] for ans in data.get('Answer', []) if ans.get('type') == 1]
    except Exception as e:
        logger.debug(f"DoHæŸ¥è¯¢å¤±è´¥ {domain} @ {doh_server}: {e}")
    return []

def query_dns_doh_all(domain: str) -> List[str]:
    """ä»æ‰€æœ‰DoHæœåŠ¡å™¨æŸ¥è¯¢"""
    all_ips = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=len(DOH_SERVERS)) as executor:
        futures = [executor.submit(query_dns_doh, domain, srv) for srv in DOH_SERVERS]
        for future in concurrent.futures.as_completed(futures):
            try:
                all_ips.extend(future.result())
            except:
                pass
    return list(set(all_ips))

# ==================== ä¼ ç»ŸDNSæŸ¥è¯¢ ====================

def query_dns_traditional(domain: str, dns_server: str) -> List[str]:
    """ä¼ ç»ŸDNSæŸ¥è¯¢"""
    try:
        resolver = dns.resolver.Resolver()
        resolver.nameservers = [dns_server]
        resolver.timeout = 3
        resolver.lifetime = 3
        return [str(rdata) for rdata in resolver.resolve(domain, 'A')]
    except Exception as e:
        logger.debug(f"DNSæŸ¥è¯¢å¤±è´¥ {domain} @ {dns_server}: {e}")
    return []

def query_dns_traditional_all(domain: str) -> List[str]:
    """ä»æ‰€æœ‰ä¼ ç»ŸDNSæŸ¥è¯¢"""
    all_ips = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=len(DNS_SERVERS)) as executor:
        futures = [executor.submit(query_dns_traditional, domain, srv) for srv in DNS_SERVERS]
        for future in concurrent.futures.as_completed(futures):
            try:
                all_ips.extend(future.result())
            except:
                pass
    unique_ips = list(set(all_ips))
    return [ip for ip in unique_ips if not ip.startswith(('127.', '0.', '169.254.'))]

# ==================== Webçˆ¬è™«é™çº§ ====================

def query_ipaddress_com(domain: str) -> List[str]:
    """ä»ipaddress.comçˆ¬å–IPï¼ˆç¬¬ä¸‰å±‚é™çº§ï¼‰"""
    try:
        import requests
        import re
        url = f'https://sites.ipaddress.com/{domain}'
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=5)
        if response.status_code == 200:
            pattern = r"\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b"
            ips = re.findall(pattern, response.text)
            # è¿‡æ»¤æ˜æ˜¾é”™è¯¯çš„IP
            valid_ips = [ip for ip in ips if not ip.startswith(('127.', '0.', '255.', '169.254.'))]
            return list(set(valid_ips))[:5]  # æœ€å¤šè¿”å›5ä¸ª
    except Exception as e:
        logger.debug(f"Webçˆ¬è™«å¤±è´¥ {domain}: {e}")
    return []

# ==================== ä¸‰å±‚é™çº§DNSæŸ¥è¯¢ ====================

def get_all_ips(domain: str, use_doh: bool = True, use_web: bool = True) -> List[str]:
    """ä¸‰å±‚é™çº§ç­–ç•¥ï¼šDoH â†’ ä¼ ç»ŸDNS â†’ Webçˆ¬è™«"""
    # Layer 1: DoH
    if use_doh:
        try:
            ips = query_dns_doh_all(domain)
            if ips:
                logger.debug(f"{domain} - DoHæˆåŠŸ: {len(ips)}ä¸ªIP")
                return ips
        except:
            pass

    # Layer 2: ä¼ ç»ŸDNS
    try:
        ips = query_dns_traditional_all(domain)
        if ips:
            logger.debug(f"{domain} - ä¼ ç»ŸDNSæˆåŠŸ: {len(ips)}ä¸ªIP")
            return ips
    except:
        pass

    # Layer 3: Webçˆ¬è™«
    if use_web:
        try:
            ips = query_ipaddress_com(domain)
            if ips:
                logger.debug(f"{domain} - Webçˆ¬è™«æˆåŠŸ: {len(ips)}ä¸ªIP")
                return ips
        except:
            pass

    return []

# ==================== TCPæµ‹é€Ÿ ====================

def test_tcp_speed(ip: str, port: int = TCP_PORT, timeout: int = TCP_TIMEOUT) -> float:
    """æµ‹è¯•TCPè¿æ¥é€Ÿåº¦"""
    try:
        start = time.time()
        sock = socket.create_connection((ip, port), timeout=timeout)
        sock.close()
        return (time.time() - start) * 1000
    except:
        return float('inf')

def test_tcp_latency(ip: str, count: int = TCP_TEST_COUNT) -> float:
    """å¤šæ¬¡TCPæµ‹è¯•å–ä¸­ä½æ•°"""
    results = [test_tcp_speed(ip) for _ in range(count)]
    results = [r for r in results if r != float('inf')]
    if not results:
        return float('inf')
    results.sort()
    mid = len(results) // 2
    if len(results) % 2 == 0:
        return (results[mid - 1] + results[mid]) / 2
    return results[mid]

# ==================== æ™ºèƒ½ç¼“å­˜ ====================

def load_cache() -> Dict:
    """åŠ è½½ç¼“å­˜"""
    if not CACHE_ENABLED or not os.path.exists(CACHE_FILE):
        return {}
    try:
        with open(CACHE_FILE, 'r') as f:
            return json.load(f)
    except:
        return {}

def save_cache(cache: Dict):
    """ä¿å­˜ç¼“å­˜"""
    if not CACHE_ENABLED:
        return
    try:
        with open(CACHE_FILE, 'w') as f:
            json.dump(cache, f, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

def update_cache(domain: str, ip: str, latency: float):
    """æ›´æ–°ç¼“å­˜ï¼ˆæ»‘åŠ¨å¹³å‡ï¼‰"""
    if not CACHE_ENABLED:
        return
    cache = load_cache()
    key = f"{domain}:{ip}"
    if key not in cache:
        cache[key] = {'count': 0, 'avg_latency': 0, 'last_success': None}
    old_avg = cache[key]['avg_latency']
    count = cache[key]['count']
    cache[key]['avg_latency'] = (old_avg * count + latency) / (count + 1)
    cache[key]['count'] += 1
    cache[key]['last_success'] = datetime.now().isoformat()
    save_cache(cache)

def get_cached_ips(domain: str) -> List[Tuple[str, float]]:
    """è·å–ç¼“å­˜çš„IP"""
    cache = load_cache()
    results = []
    for key, data in cache.items():
        if key.startswith(f"{domain}:"):
            ip = key.split(':', 1)[1]
            results.append((ip, data['avg_latency']))
    return results

# ==================== æ ¸å¿ƒå¤„ç† ====================

def get_fastest_ips(domain: str, use_doh: bool = True, use_cache: bool = True, use_web: bool = True) -> List[Tuple[str, float]]:
    """è·å–æœ€å¿«çš„å‰Nä¸ªIP"""
    logger.info(f"å¤„ç†: {domain}")

    # è·å–IPåˆ—è¡¨
    ips = get_all_ips(domain, use_doh, use_web)
    if use_cache:
        cached = get_cached_ips(domain)
        ips = list(set(ips + [ip for ip, _ in cached]))

    if not ips:
        logger.warning(f"{domain} - æœªæ‰¾åˆ°IP")
        return []

    logger.debug(f"{domain} - æ‰¾åˆ°{len(ips)}ä¸ªIP")

    # å¹¶å‘æµ‹é€Ÿ
    results = {}
    with concurrent.futures.ThreadPoolExecutor(max_workers=min(len(ips), 5)) as executor:
        future_to_ip = {executor.submit(test_tcp_latency, ip): ip for ip in ips}
        for future in concurrent.futures.as_completed(future_to_ip):
            ip = future_to_ip[future]
            try:
                latency = future.result()
                results[ip] = latency
                if latency != float('inf'):
                    logger.debug(f"{domain} - {ip}: {latency:.2f}ms")
                    update_cache(domain, ip, latency)
            except:
                results[ip] = float('inf')

    # æ’åºå¹¶è¿”å›å‰Nä¸ª
    sorted_results = sorted(results.items(), key=lambda x: x[1])
    valid = [(ip, lat) for ip, lat in sorted_results if lat != float('inf')]

    if valid:
        top_n = valid[:TOP_IP_COUNT]
        logger.info(f"{domain} - æœ€å¿«: {', '.join([f'{ip}({lat:.0f}ms)' for ip, lat in top_n])}")
        return top_n
    elif ips:
        logger.warning(f"{domain} - æ‰€æœ‰IPæµ‹é€Ÿå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤: {ips[0]}")
        return [(ips[0], float('inf'))]
    return []

# ==================== æ–‡ä»¶ç”Ÿæˆ ====================

def get_domain_list(level: str) -> List[str]:
    """è·å–åŸŸååˆ—è¡¨(ä»ALL_DOMAINSä¸­æŒ‰æ•°é‡åˆ‡ç‰‡)"""
    count = DOMAIN_COUNT.get(level, DOMAIN_COUNT['core'])
    return ALL_DOMAINS[:count]

def generate_hosts_content(results: Dict, level: str, multi_ip: bool = True) -> str:
    """ç”Ÿæˆhostsæ–‡ä»¶å†…å®¹"""
    domains = get_domain_list(level)
    lines = [
        "# GitHub Hosts Ultimate",
        f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"# ç‰ˆæœ¬: {VERSION}",
        f"# åŸŸåçº§åˆ«: {level.upper()}",
        f"# æˆåŠŸç‡: {len(results)}/{len(domains)} ({len(results)/len(domains)*100:.1f}%)",
        "#",
        "# ==================== GitHub Hosts Start ====================",
        ""
    ]

    for domain in domains:
        if domain in results:
            ip_list = results[domain]
            if multi_ip:
                for ip, latency in ip_list:
                    latency_str = f"# {latency:.1f}ms" if latency != float('inf') else "# timeout"
                    lines.append(f"{ip:<20} {domain:<50} {latency_str}")
            else:
                ip, latency = ip_list[0]
                latency_str = f"# {latency:.1f}ms" if latency != float('inf') else "# timeout"
                lines.append(f"{ip:<20} {domain:<50} {latency_str}")

    lines.extend(["", "# ==================== GitHub Hosts End ====================", ""])
    return '\n'.join(lines)

def generate_hosts_file(output_file: str, level: str, use_doh: bool, use_cache: bool, use_web: bool, multi_ip: bool):
    """ç”Ÿæˆhostsæ–‡ä»¶"""
    domains = get_domain_list(level)
    logger.info("=" * 70)
    logger.info(f"{PROGRAM_NAME} v{VERSION}")
    logger.info("=" * 70)
    logger.info(f"åŸŸåçº§åˆ«: {level.upper()} ({len(domains)}ä¸ª)")
    logger.info(f"DNSæ–¹å¼: {'DoH' if use_doh else 'ä¼ ç»ŸDNS'}")
    logger.info(f"æ™ºèƒ½ç¼“å­˜: {'å¯ç”¨' if use_cache else 'ç¦ç”¨'}")
    logger.info(f"Webé™çº§: {'å¯ç”¨' if use_web else 'ç¦ç”¨'}")
    logger.info(f"å¤šIPè½®è¯¢: {'å¯ç”¨' if multi_ip else 'ç¦ç”¨'}")
    logger.info("=" * 70)

    start_time = time.time()
    results = {}
    success_count = 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_domain = {
            executor.submit(get_fastest_ips, domain, use_doh, use_cache, use_web): domain
            for domain in domains
        }
        for future in concurrent.futures.as_completed(future_to_domain):
            domain = future_to_domain[future]
            try:
                ip_list = future.result()
                if ip_list:
                    results[domain] = ip_list
                    success_count += 1
            except Exception as e:
                logger.error(f"å¤„ç†å¤±è´¥ {domain}: {e}")

    # ç”Ÿæˆå†…å®¹
    content = generate_hosts_content(results, level, multi_ip)

    # å†™å…¥æ–‡ä»¶
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)
    except Exception as e:
        logger.error(f"å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")
        return

    elapsed = time.time() - start_time

    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_domains': len(domains),
        'success_count': success_count,
        'success_rate': success_count / len(domains) * 100,
        'elapsed_time': elapsed,
        'timestamp': datetime.now().isoformat(),
        'level': level,
        'use_doh': use_doh,
        'use_cache': use_cache,
        'multi_ip': multi_ip
    }

    logger.info("=" * 70)
    logger.info(f"âœ… Hostsæ–‡ä»¶å·²ç”Ÿæˆ: {output_file}")
    logger.info(f"âœ… æˆåŠŸç‡: {success_count}/{len(domains)} ({stats['success_rate']:.1f}%)")
    logger.info(f"â±ï¸  æ€»è€—æ—¶: {elapsed:.2f}ç§’")
    logger.info("=" * 70)

    return results, stats

# ==================== ç»Ÿè®¡æŠ¥å‘Šç”Ÿæˆ ====================

def generate_stats_report(results: Dict, stats: Dict, output_file: str = 'stats_report.md'):
    """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
    try:
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        all_latencies = []
        for domain, ip_list in results.items():
            for ip, latency in ip_list:
                if latency != float('inf'):
                    all_latencies.append((domain, ip, latency))

        all_latencies.sort(key=lambda x: x[2])

        # ç”ŸæˆæŠ¥å‘Š
        lines = [
            f"# GitHub Hosts æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š",
            f"",
            f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"",
            f"## ğŸ“Š æ€»ä½“ç»Ÿè®¡",
            f"",
            f"- **åŸŸåæ€»æ•°**: {stats['total_domains']}",
            f"- **æˆåŠŸè§£æ**: {stats['success_count']} ({stats['success_rate']:.1f}%)",
            f"- **æ€»è€—æ—¶**: {stats['elapsed_time']:.2f}ç§’",
            f"- **åŸŸåçº§åˆ«**: {stats['level'].upper()}",
            f"",
            f"## âš¡ æ€§èƒ½åˆ†æ",
            f"",
        ]

        if all_latencies:
            avg_latency = sum(l[2] for l in all_latencies) / len(all_latencies)
            min_latency = all_latencies[0][2]
            max_latency = all_latencies[-1][2]

            lines.extend([
                f"- **å¹³å‡å»¶è¿Ÿ**: {avg_latency:.2f}ms",
                f"- **æœ€ä½å»¶è¿Ÿ**: {min_latency:.2f}ms ({all_latencies[0][0]})",
                f"- **æœ€é«˜å»¶è¿Ÿ**: {max_latency:.2f}ms ({all_latencies[-1][0]})",
                f"",
                f"## ğŸ† Top 10 æœ€å¿«åŸŸå",
                f"",
                f"| æ’å | åŸŸå | IP | å»¶è¿Ÿ |",
                f"|------|------|-----|------|",
            ])

            for idx, (domain, ip, latency) in enumerate(all_latencies[:10], 1):
                lines.append(f"| {idx} | {domain} | {ip} | {latency:.2f}ms |")

            lines.extend([
                f"",
                f"## ğŸ¢ Top 10 æœ€æ…¢åŸŸå",
                f"",
                f"| æ’å | åŸŸå | IP | å»¶è¿Ÿ |",
                f"|------|------|-----|------|",
            ])

            for idx, (domain, ip, latency) in enumerate(all_latencies[-10:][::-1], 1):
                lines.append(f"| {idx} | {domain} | {ip} | {latency:.2f}ms |")

        lines.extend([
            f"",
            f"## ğŸ“ˆ å»¶è¿Ÿåˆ†å¸ƒ",
            f"",
        ])

        if all_latencies:
            ranges = [
                (0, 50, "ğŸŸ¢ ä¼˜ç§€"),
                (50, 100, "ğŸŸ¡ è‰¯å¥½"),
                (100, 200, "ğŸŸ  ä¸€èˆ¬"),
                (200, float('inf'), "ğŸ”´ è¾ƒæ…¢")
            ]

            for min_l, max_l, label in ranges:
                count = len([l for _, _, l in all_latencies if min_l <= l < max_l])
                pct = count / len(all_latencies) * 100
                lines.append(f"- **{label}** ({min_l}-{max_l}ms): {count}ä¸ª ({pct:.1f}%)")

        lines.extend([
            f"",
            f"---",
            f"",
            f"*æŠ¥å‘Šç”± {PROGRAM_NAME} v{VERSION} ç”Ÿæˆ*",
        ])

        # å†™å…¥æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

        logger.info(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")

    except Exception as e:
        logger.error(f"ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šå¤±è´¥: {e}")

# ==================== ä¸»ç¨‹åº ====================

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description=f'{PROGRAM_NAME} v{VERSION}',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ç”Ÿæˆæ‰©å±•çº§åˆ«hosts
  python %(prog)s --level=extended

  # ç”Ÿæˆå®Œæ•´çº§åˆ«hostså¹¶ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
  python %(prog)s --level=full --report

  # ç¦ç”¨DoHå’Œç¼“å­˜
  python %(prog)s --no-doh --no-cache
        """
    )

    parser.add_argument('--level', choices=['core', 'extended', 'full'], default='extended',
                        help='åŸŸåçº§åˆ« [é»˜è®¤: extended]')
    parser.add_argument('--output', default='github_hosts_ultimate',
                        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ [é»˜è®¤: github_hosts_ultimate]')
    parser.add_argument('--no-doh', action='store_true', help='ç¦ç”¨DoH')
    parser.add_argument('--no-cache', action='store_true', help='ç¦ç”¨ç¼“å­˜')
    parser.add_argument('--no-web', action='store_true', help='ç¦ç”¨Webçˆ¬è™«é™çº§')
    parser.add_argument('--no-multi-ip', action='store_true', help='ç¦ç”¨å¤šIPè½®è¯¢')
    parser.add_argument('--report', action='store_true', help='ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š')
    parser.add_argument('--version', action='version', version=f'{PROGRAM_NAME} v{VERSION}')

    args = parser.parse_args()

    # æ£€æŸ¥ä¾èµ–
    try:
        import dns.resolver
        import requests
    except ImportError as e:
        logger.error(f"ç¼ºå°‘ä¾èµ–: {e}")
        logger.error("è¯·å®‰è£…: pip install dnspython requests")
        sys.exit(1)

    try:
        logger.info(f"ğŸš€ å¯åŠ¨ {PROGRAM_NAME} v{VERSION}")

        results, stats = generate_hosts_file(
            args.output,
            args.level,
            not args.no_doh,
            not args.no_cache,
            not args.no_web,
            not args.no_multi_ip
        )

        if args.report:
            generate_stats_report(results, stats, 'stats_report.md')

    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        logger.error(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == '__main__':
    main()
