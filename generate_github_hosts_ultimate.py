#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GitHub Hosts è‡ªåŠ¨ç”Ÿæˆå·¥å…· - Ultimateç‰ˆæœ¬
åŠŸèƒ½ï¼šDoHæŸ¥è¯¢ + çº¯TCPæµ‹é€Ÿ + æ™ºèƒ½ç¼“å­˜ + åŸŸååˆ†çº§ + Daemonæ¨¡å¼ + HTTP API + ç»Ÿè®¡æŠ¥å‘Š
ä½œè€…ï¼šåŸºäºProç‰ˆæœ¬ç»ˆæä¼˜åŒ–
ç‰ˆæœ¬ï¼š3.0.0 Ultimate
"""

import dns.resolver
import socket
import concurrent.futures
import time
import json
import argparse
import os
import logging
import signal
import sys
import threading
from datetime import datetime
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.parse import urlparse, parse_qs

# ==================== ç‰ˆæœ¬ä¿¡æ¯ ====================
VERSION = "3.0.0"
PROGRAM_NAME = "GitHub Hosts Ultimate"

# ==================== æ—¥å¿—é…ç½® ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('github_hosts_ultimate.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==================== é…ç½®åŒº ====================

# åŸŸååˆ†çº§é…ç½®
DOMAIN_LEVELS = {
    'core': [
        # ===== æ ¸å¿ƒæœåŠ¡ï¼ˆ20ä¸ªï¼‰=====
        'github.com', 'api.github.com', 'gist.github.com', 'codeload.github.com',
        'github.blog', 'github.community', 'github.dev', 'alive.github.com',
        'live.github.com', 'education.github.com',
        # ===== CDNä¸é™æ€èµ„æºï¼ˆ5ä¸ªï¼‰=====
        'github.githubassets.com', 'github.io', 'github.map.fastly.net',
        'github.global.ssl.fastly.net', 'githubstatus.com',
        # ===== UserContentæ ¸å¿ƒï¼ˆ5ä¸ªï¼‰=====
        'raw.githubusercontent.com', 'objects.githubusercontent.com',
        'avatars.githubusercontent.com', 'camo.githubusercontent.com',
        'user-images.githubusercontent.com',
    ],
    'extended': [
        # ===== æ›´å¤šUserContentï¼ˆ16ä¸ªï¼‰=====
        'raw.github.com', 'objects-origin.githubusercontent.com',
        'release-assets.githubusercontent.com', 'github-releases.githubusercontent.com',
        'github-registry-files.githubusercontent.com', 'avatars0.githubusercontent.com',
        'avatars1.githubusercontent.com', 'avatars2.githubusercontent.com',
        'avatars3.githubusercontent.com', 'avatars4.githubusercontent.com',
        'avatars5.githubusercontent.com', 'private-user-images.githubusercontent.com',
        'cloud.githubusercontent.com', 'desktop.githubusercontent.com',
        'favicons.githubusercontent.com', 'media.githubusercontent.com',
        'pkg-containers.githubusercontent.com',
        # ===== åŒ…ç®¡ç†å™¨ï¼ˆ13ä¸ªï¼‰=====
        'ghcr.io', 'maven.pkg.github.com', 'npm.pkg.github.com',
        'npm-proxy.pkg.github.com', 'npm-beta.pkg.github.com',
        'npm-beta-proxy.pkg.github.com', 'nuget.pkg.github.com',
        'rubygems.pkg.github.com', 'pypi.pkg.github.com',
        'swift.pkg.github.com', 'docker.pkg.github.com',
        'docker-proxy.pkg.github.com', 'containers.pkg.github.com',
        # ===== AWS S3ï¼ˆ5ä¸ªï¼‰=====
        'github-cloud.s3.amazonaws.com', 'github-com.s3.amazonaws.com',
        'github-production-release-asset-2e65be.s3.amazonaws.com',
        'github-production-user-asset-6210df.s3.amazonaws.com',
        'github-production-repository-file-5c1aeb.s3.amazonaws.com',
        # ===== Copilotï¼ˆ7ä¸ªï¼‰=====
        'githubcopilot.com', 'api.githubcopilot.com',
        'api.individual.githubcopilot.com', 'copilot-proxy.githubusercontent.com',
        'copilot-telemetry.githubusercontent.com', 'default.exp-tas.com',
        'collector.github.com', 'central.github.com',
    ],
    'optional': [
        # ===== Actionsï¼ˆ49ä¸ªï¼‰=====
        'pipelines.actions.githubusercontent.com', 'vstoken.actions.githubusercontent.com',
        'broker.actions.githubusercontent.com', 'launch.actions.githubusercontent.com',
        'runner-auth.actions.githubusercontent.com', 'tokenghub.actions.githubusercontent.com',
        'setup-tools.actions.githubusercontent.com', 'pkg.actions.githubusercontent.com',
        'results-receiver.actions.githubusercontent.com', 'mpsghub.actions.githubusercontent.com',
        'pipelinesghubeus1.actions.githubusercontent.com', 'pipelinesghubeus2.actions.githubusercontent.com',
        'pipelinesghubeus3.actions.githubusercontent.com', 'pipelinesghubeus4.actions.githubusercontent.com',
        'pipelinesghubeus5.actions.githubusercontent.com', 'pipelinesghubeus6.actions.githubusercontent.com',
        'pipelinesghubeus7.actions.githubusercontent.com', 'pipelinesghubeus8.actions.githubusercontent.com',
        'pipelinesghubeus9.actions.githubusercontent.com', 'pipelinesghubeus10.actions.githubusercontent.com',
        'pipelinesghubeus11.actions.githubusercontent.com', 'pipelinesghubeus12.actions.githubusercontent.com',
        'pipelinesghubeus13.actions.githubusercontent.com', 'pipelinesghubeus14.actions.githubusercontent.com',
        'pipelinesghubeus15.actions.githubusercontent.com', 'pipelinesghubeus20.actions.githubusercontent.com',
        'pipelinesghubeus21.actions.githubusercontent.com', 'pipelinesghubeus22.actions.githubusercontent.com',
        'pipelinesghubeus23.actions.githubusercontent.com', 'pipelinesghubeus24.actions.githubusercontent.com',
        'pipelinesghubeus25.actions.githubusercontent.com', 'pipelinesghubeus26.actions.githubusercontent.com',
        'pipelinesproxcnc1.actions.githubusercontent.com', 'pipelinesproxcus1.actions.githubusercontent.com',
        'pipelinesproxeau1.actions.githubusercontent.com', 'pipelinesproxsdc1.actions.githubusercontent.com',
        'pipelinesproxweu1.actions.githubusercontent.com', 'pipelinesproxwus31.actions.githubusercontent.com',
        'runnerghubeus1.actions.githubusercontent.com', 'runnerghubeus20.actions.githubusercontent.com',
        'runnerghubeus21.actions.githubusercontent.com', 'runnerghubwus31.actions.githubusercontent.com',
        'runnerproxcnc1.actions.githubusercontent.com', 'runnerproxcus1.actions.githubusercontent.com',
        'runnerproxeau1.actions.githubusercontent.com', 'runnerproxsdc1.actions.githubusercontent.com',
        'runnerproxweu1.actions.githubusercontent.com', 'run-actions-1-azure-eastus.actions.githubusercontent.com',
        'run-actions-2-azure-eastus.actions.githubusercontent.com', 'run-actions-3-azure-eastus.actions.githubusercontent.com',
        # ===== Azure Blobï¼ˆ24ä¸ªï¼‰=====
        'productionresultssa0.blob.core.windows.net', 'productionresultssa1.blob.core.windows.net',
        'productionresultssa2.blob.core.windows.net', 'productionresultssa3.blob.core.windows.net',
        'productionresultssa4.blob.core.windows.net', 'productionresultssa5.blob.core.windows.net',
        'productionresultssa6.blob.core.windows.net', 'productionresultssa7.blob.core.windows.net',
        'productionresultssa8.blob.core.windows.net', 'productionresultssa9.blob.core.windows.net',
        'productionresultssa10.blob.core.windows.net', 'productionresultssa11.blob.core.windows.net',
        'productionresultssa12.blob.core.windows.net', 'productionresultssa13.blob.core.windows.net',
        'productionresultssa14.blob.core.windows.net', 'productionresultssa15.blob.core.windows.net',
        'productionresultssa16.blob.core.windows.net', 'productionresultssa17.blob.core.windows.net',
        'productionresultssa18.blob.core.windows.net', 'productionresultssa19.blob.core.windows.net',
        'mavenregistryv2prod.blob.core.windows.net', 'npmregistryv2prod.blob.core.windows.net',
        'nugetregistryv2prod.blob.core.windows.net', 'rubygemsregistryv2prod.blob.core.windows.net',
        # ===== å…¶ä»–ï¼ˆ4ä¸ªï¼‰=====
        'tuf-repo.github.com', 'fulcio.githubapp.com',
        'timestamp.githubapp.com', 'vscode.dev',
    ]
}

# DoHæœåŠ¡å™¨
DOH_SERVERS = [
    'https://1.1.1.1/dns-query',
    'https://8.8.8.8/resolve',
    'https://223.5.5.5/resolve',
]

# ä¼ ç»ŸDNSæœåŠ¡å™¨
DNS_SERVERS = ['1.1.1.1', '8.8.8.8', '223.5.5.5', '114.114.114.114']

# æµ‹é€Ÿé…ç½®
TCP_TEST_COUNT = 3
TCP_TIMEOUT = 2
TCP_PORT = 443
MAX_WORKERS = 10
TOP_IP_COUNT = 3

# ç¼“å­˜é…ç½®
CACHE_FILE = '.github_hosts_cache.json'
CACHE_ENABLED = True

# Daemoné…ç½®
DEFAULT_DAEMON_INTERVAL = 600  # 10åˆ†é’Ÿ
DEFAULT_HTTP_PORT = 8080

# å…¨å±€çŠ¶æ€
class GlobalState:
    """å…¨å±€çŠ¶æ€ç®¡ç†"""
    def __init__(self):
        self.results = {}
        self.stats = {}
        self.last_update = None
        self.is_running = True
        self.lock = threading.Lock()

    def update_results(self, results):
        with self.lock:
            self.results = results
            self.last_update = datetime.now()

    def get_results(self):
        with self.lock:
            return self.results.copy()

    def update_stats(self, stats):
        with self.lock:
            self.stats = stats

    def get_stats(self):
        with self.lock:
            return self.stats.copy()

    def stop(self):
        self.is_running = False

global_state = GlobalState()

# ==================== DoHæŸ¥è¯¢æ¨¡å— ====================

def query_dns_doh(domain: str, doh_server: str) -> List[str]:
    """ä½¿ç”¨DNS-over-HTTPSæŸ¥è¯¢"""
    try:
        import requests
        response = requests.get(
            doh_server,
            params={'name': domain, 'type': 'A'},
            headers={'accept': 'application/dns-json'},
            timeout=3
        )
        if response.status_code == 200:
            data = response.json()
            return [ans['data'] for ans in data.get('Answer', []) if ans.get('type') == 1]
    except Exception as e:
        logger.debug(f"DoHæŸ¥è¯¢å¤±è´¥ {domain} @ {doh_server}: {e}")
    return []

def query_dns_doh_all(domain: str) -> List[str]:
    """ä»æ‰€æœ‰DoHæœåŠ¡å™¨æŸ¥è¯¢"""
    all_ips = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=len(DOH_SERVERS)) as executor:
        futures = [executor.submit(query_dns_doh, domain, srv) for srv in DOH_SERVERS]
        for future in concurrent.futures.as_completed(futures):
            try:
                all_ips.extend(future.result())
            except:
                pass
    return list(set(all_ips))

# ==================== ä¼ ç»ŸDNSæŸ¥è¯¢ ====================

def query_dns_traditional(domain: str, dns_server: str) -> List[str]:
    """ä¼ ç»ŸDNSæŸ¥è¯¢"""
    try:
        resolver = dns.resolver.Resolver()
        resolver.nameservers = [dns_server]
        resolver.timeout = 3
        resolver.lifetime = 3
        return [str(rdata) for rdata in resolver.resolve(domain, 'A')]
    except Exception as e:
        logger.debug(f"DNSæŸ¥è¯¢å¤±è´¥ {domain} @ {dns_server}: {e}")
    return []

def query_dns_traditional_all(domain: str) -> List[str]:
    """ä»æ‰€æœ‰ä¼ ç»ŸDNSæŸ¥è¯¢"""
    all_ips = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=len(DNS_SERVERS)) as executor:
        futures = [executor.submit(query_dns_traditional, domain, srv) for srv in DNS_SERVERS]
        for future in concurrent.futures.as_completed(futures):
            try:
                all_ips.extend(future.result())
            except:
                pass
    unique_ips = list(set(all_ips))
    return [ip for ip in unique_ips if not ip.startswith(('127.', '0.', '169.254.'))]

# ==================== Webçˆ¬è™«é™çº§ ====================

def query_ipaddress_com(domain: str) -> List[str]:
    """ä»ipaddress.comçˆ¬å–IPï¼ˆç¬¬ä¸‰å±‚é™çº§ï¼‰"""
    try:
        import requests
        import re
        url = f'https://sites.ipaddress.com/{domain}'
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=5)
        if response.status_code == 200:
            pattern = r"\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b"
            ips = re.findall(pattern, response.text)
            # è¿‡æ»¤æ˜æ˜¾é”™è¯¯çš„IP
            valid_ips = [ip for ip in ips if not ip.startswith(('127.', '0.', '255.', '169.254.'))]
            return list(set(valid_ips))[:5]  # æœ€å¤šè¿”å›5ä¸ª
    except Exception as e:
        logger.debug(f"Webçˆ¬è™«å¤±è´¥ {domain}: {e}")
    return []

# ==================== ä¸‰å±‚é™çº§DNSæŸ¥è¯¢ ====================

def get_all_ips(domain: str, use_doh: bool = True, use_web: bool = True) -> List[str]:
    """ä¸‰å±‚é™çº§ç­–ç•¥ï¼šDoH â†’ ä¼ ç»ŸDNS â†’ Webçˆ¬è™«"""
    # Layer 1: DoH
    if use_doh:
        try:
            ips = query_dns_doh_all(domain)
            if ips:
                logger.debug(f"{domain} - DoHæˆåŠŸ: {len(ips)}ä¸ªIP")
                return ips
        except:
            pass

    # Layer 2: ä¼ ç»ŸDNS
    try:
        ips = query_dns_traditional_all(domain)
        if ips:
            logger.debug(f"{domain} - ä¼ ç»ŸDNSæˆåŠŸ: {len(ips)}ä¸ªIP")
            return ips
    except:
        pass

    # Layer 3: Webçˆ¬è™«
    if use_web:
        try:
            ips = query_ipaddress_com(domain)
            if ips:
                logger.debug(f"{domain} - Webçˆ¬è™«æˆåŠŸ: {len(ips)}ä¸ªIP")
                return ips
        except:
            pass

    return []

# ==================== TCPæµ‹é€Ÿ ====================

def test_tcp_speed(ip: str, port: int = TCP_PORT, timeout: int = TCP_TIMEOUT) -> float:
    """æµ‹è¯•TCPè¿æ¥é€Ÿåº¦"""
    try:
        start = time.time()
        sock = socket.create_connection((ip, port), timeout=timeout)
        sock.close()
        return (time.time() - start) * 1000
    except:
        return float('inf')

def test_tcp_latency(ip: str, count: int = TCP_TEST_COUNT) -> float:
    """å¤šæ¬¡TCPæµ‹è¯•å–ä¸­ä½æ•°"""
    results = [test_tcp_speed(ip) for _ in range(count)]
    results = [r for r in results if r != float('inf')]
    if not results:
        return float('inf')
    results.sort()
    mid = len(results) // 2
    if len(results) % 2 == 0:
        return (results[mid - 1] + results[mid]) / 2
    return results[mid]

# ==================== æ™ºèƒ½ç¼“å­˜ ====================

def load_cache() -> Dict:
    """åŠ è½½ç¼“å­˜"""
    if not CACHE_ENABLED or not os.path.exists(CACHE_FILE):
        return {}
    try:
        with open(CACHE_FILE, 'r') as f:
            return json.load(f)
    except:
        return {}

def save_cache(cache: Dict):
    """ä¿å­˜ç¼“å­˜"""
    if not CACHE_ENABLED:
        return
    try:
        with open(CACHE_FILE, 'w') as f:
            json.dump(cache, f, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

def update_cache(domain: str, ip: str, latency: float):
    """æ›´æ–°ç¼“å­˜ï¼ˆæ»‘åŠ¨å¹³å‡ï¼‰"""
    if not CACHE_ENABLED:
        return
    cache = load_cache()
    key = f"{domain}:{ip}"
    if key not in cache:
        cache[key] = {'count': 0, 'avg_latency': 0, 'last_success': None}
    old_avg = cache[key]['avg_latency']
    count = cache[key]['count']
    cache[key]['avg_latency'] = (old_avg * count + latency) / (count + 1)
    cache[key]['count'] += 1
    cache[key]['last_success'] = datetime.now().isoformat()
    save_cache(cache)

def get_cached_ips(domain: str) -> List[Tuple[str, float]]:
    """è·å–ç¼“å­˜çš„IP"""
    cache = load_cache()
    results = []
    for key, data in cache.items():
        if key.startswith(f"{domain}:"):
            ip = key.split(':', 1)[1]
            results.append((ip, data['avg_latency']))
    return results

# ==================== æ ¸å¿ƒå¤„ç† ====================

def get_fastest_ips(domain: str, use_doh: bool = True, use_cache: bool = True, use_web: bool = True) -> List[Tuple[str, float]]:
    """è·å–æœ€å¿«çš„å‰Nä¸ªIP"""
    logger.info(f"å¤„ç†: {domain}")

    # è·å–IPåˆ—è¡¨
    ips = get_all_ips(domain, use_doh, use_web)
    if use_cache:
        cached = get_cached_ips(domain)
        ips = list(set(ips + [ip for ip, _ in cached]))

    if not ips:
        logger.warning(f"{domain} - æœªæ‰¾åˆ°IP")
        return []

    logger.debug(f"{domain} - æ‰¾åˆ°{len(ips)}ä¸ªIP")

    # å¹¶å‘æµ‹é€Ÿ
    results = {}
    with concurrent.futures.ThreadPoolExecutor(max_workers=min(len(ips), 5)) as executor:
        future_to_ip = {executor.submit(test_tcp_latency, ip): ip for ip in ips}
        for future in concurrent.futures.as_completed(future_to_ip):
            ip = future_to_ip[future]
            try:
                latency = future.result()
                results[ip] = latency
                if latency != float('inf'):
                    logger.debug(f"{domain} - {ip}: {latency:.2f}ms")
                    update_cache(domain, ip, latency)
            except:
                results[ip] = float('inf')

    # æ’åºå¹¶è¿”å›å‰Nä¸ª
    sorted_results = sorted(results.items(), key=lambda x: x[1])
    valid = [(ip, lat) for ip, lat in sorted_results if lat != float('inf')]

    if valid:
        top_n = valid[:TOP_IP_COUNT]
        logger.info(f"{domain} - æœ€å¿«: {', '.join([f'{ip}({lat:.0f}ms)' for ip, lat in top_n])}")
        return top_n
    elif ips:
        logger.warning(f"{domain} - æ‰€æœ‰IPæµ‹é€Ÿå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤: {ips[0]}")
        return [(ips[0], float('inf'))]
    return []

# ==================== æ–‡ä»¶ç”Ÿæˆ ====================

def get_domain_list(level: str) -> List[str]:
    """è·å–åŸŸååˆ—è¡¨"""
    if level == 'core':
        return DOMAIN_LEVELS['core']
    elif level == 'extended':
        return DOMAIN_LEVELS['core'] + DOMAIN_LEVELS['extended']
    else:  # full
        return DOMAIN_LEVELS['core'] + DOMAIN_LEVELS['extended'] + DOMAIN_LEVELS['optional']

def generate_hosts_content(results: Dict, level: str, multi_ip: bool = True) -> str:
    """ç”Ÿæˆhostsæ–‡ä»¶å†…å®¹"""
    domains = get_domain_list(level)
    lines = [
        "# GitHub Hosts Ultimate",
        f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"# ç‰ˆæœ¬: {VERSION}",
        f"# åŸŸåçº§åˆ«: {level.upper()}",
        f"# æˆåŠŸç‡: {len(results)}/{len(domains)} ({len(results)/len(domains)*100:.1f}%)",
        "#",
        "# ==================== GitHub Hosts Start ====================",
        ""
    ]

    for domain in domains:
        if domain in results:
            ip_list = results[domain]
            if multi_ip:
                for ip, latency in ip_list:
                    latency_str = f"# {latency:.1f}ms" if latency != float('inf') else "# timeout"
                    lines.append(f"{ip:<20} {domain:<50} {latency_str}")
            else:
                ip, latency = ip_list[0]
                latency_str = f"# {latency:.1f}ms" if latency != float('inf') else "# timeout"
                lines.append(f"{ip:<20} {domain:<50} {latency_str}")

    lines.extend(["", "# ==================== GitHub Hosts End ====================", ""])
    return '\n'.join(lines)

def generate_hosts_file(output_file: str, level: str, use_doh: bool, use_cache: bool, use_web: bool, multi_ip: bool):
    """ç”Ÿæˆhostsæ–‡ä»¶"""
    domains = get_domain_list(level)
    logger.info("=" * 70)
    logger.info(f"{PROGRAM_NAME} v{VERSION}")
    logger.info("=" * 70)
    logger.info(f"åŸŸåçº§åˆ«: {level.upper()} ({len(domains)}ä¸ª)")
    logger.info(f"DNSæ–¹å¼: {'DoH' if use_doh else 'ä¼ ç»ŸDNS'}")
    logger.info(f"æ™ºèƒ½ç¼“å­˜: {'å¯ç”¨' if use_cache else 'ç¦ç”¨'}")
    logger.info(f"Webé™çº§: {'å¯ç”¨' if use_web else 'ç¦ç”¨'}")
    logger.info(f"å¤šIPè½®è¯¢: {'å¯ç”¨' if multi_ip else 'ç¦ç”¨'}")
    logger.info("=" * 70)

    start_time = time.time()
    results = {}
    success_count = 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_domain = {
            executor.submit(get_fastest_ips, domain, use_doh, use_cache, use_web): domain
            for domain in domains
        }
        for future in concurrent.futures.as_completed(future_to_domain):
            domain = future_to_domain[future]
            try:
                ip_list = future.result()
                if ip_list:
                    results[domain] = ip_list
                    success_count += 1
            except Exception as e:
                logger.error(f"å¤„ç†å¤±è´¥ {domain}: {e}")

    # ç”Ÿæˆå†…å®¹
    content = generate_hosts_content(results, level, multi_ip)

    # å†™å…¥æ–‡ä»¶
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)
    except Exception as e:
        logger.error(f"å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")
        return

    elapsed = time.time() - start_time

    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_domains': len(domains),
        'success_count': success_count,
        'success_rate': success_count / len(domains) * 100,
        'elapsed_time': elapsed,
        'timestamp': datetime.now().isoformat(),
        'level': level,
        'use_doh': use_doh,
        'use_cache': use_cache,
        'multi_ip': multi_ip
    }

    global_state.update_results(results)
    global_state.update_stats(stats)

    logger.info("=" * 70)
    logger.info(f"âœ… Hostsæ–‡ä»¶å·²ç”Ÿæˆ: {output_file}")
    logger.info(f"âœ… æˆåŠŸç‡: {success_count}/{len(domains)} ({stats['success_rate']:.1f}%)")
    logger.info(f"â±ï¸  æ€»è€—æ—¶: {elapsed:.2f}ç§’")
    logger.info("=" * 70)

    return results, stats

# ==================== ç»Ÿè®¡æŠ¥å‘Šç”Ÿæˆ ====================

def generate_stats_report(results: Dict, stats: Dict, output_file: str = 'stats_report.md'):
    """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
    try:
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        all_latencies = []
        for domain, ip_list in results.items():
            for ip, latency in ip_list:
                if latency != float('inf'):
                    all_latencies.append((domain, ip, latency))

        all_latencies.sort(key=lambda x: x[2])

        # ç”ŸæˆæŠ¥å‘Š
        lines = [
            f"# GitHub Hosts æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š",
            f"",
            f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"",
            f"## ğŸ“Š æ€»ä½“ç»Ÿè®¡",
            f"",
            f"- **åŸŸåæ€»æ•°**: {stats['total_domains']}",
            f"- **æˆåŠŸè§£æ**: {stats['success_count']} ({stats['success_rate']:.1f}%)",
            f"- **æ€»è€—æ—¶**: {stats['elapsed_time']:.2f}ç§’",
            f"- **åŸŸåçº§åˆ«**: {stats['level'].upper()}",
            f"",
            f"## âš¡ æ€§èƒ½åˆ†æ",
            f"",
        ]

        if all_latencies:
            avg_latency = sum(l[2] for l in all_latencies) / len(all_latencies)
            min_latency = all_latencies[0][2]
            max_latency = all_latencies[-1][2]

            lines.extend([
                f"- **å¹³å‡å»¶è¿Ÿ**: {avg_latency:.2f}ms",
                f"- **æœ€ä½å»¶è¿Ÿ**: {min_latency:.2f}ms ({all_latencies[0][0]})",
                f"- **æœ€é«˜å»¶è¿Ÿ**: {max_latency:.2f}ms ({all_latencies[-1][0]})",
                f"",
                f"## ğŸ† Top 10 æœ€å¿«åŸŸå",
                f"",
                f"| æ’å | åŸŸå | IP | å»¶è¿Ÿ |",
                f"|------|------|-----|------|",
            ])

            for idx, (domain, ip, latency) in enumerate(all_latencies[:10], 1):
                lines.append(f"| {idx} | {domain} | {ip} | {latency:.2f}ms |")

            lines.extend([
                f"",
                f"## ğŸ¢ Top 10 æœ€æ…¢åŸŸå",
                f"",
                f"| æ’å | åŸŸå | IP | å»¶è¿Ÿ |",
                f"|------|------|-----|------|",
            ])

            for idx, (domain, ip, latency) in enumerate(all_latencies[-10:][::-1], 1):
                lines.append(f"| {idx} | {domain} | {ip} | {latency:.2f}ms |")

        lines.extend([
            f"",
            f"## ğŸ“ˆ å»¶è¿Ÿåˆ†å¸ƒ",
            f"",
        ])

        if all_latencies:
            ranges = [
                (0, 50, "ğŸŸ¢ ä¼˜ç§€"),
                (50, 100, "ğŸŸ¡ è‰¯å¥½"),
                (100, 200, "ğŸŸ  ä¸€èˆ¬"),
                (200, float('inf'), "ğŸ”´ è¾ƒæ…¢")
            ]

            for min_l, max_l, label in ranges:
                count = len([l for _, _, l in all_latencies if min_l <= l < max_l])
                pct = count / len(all_latencies) * 100
                lines.append(f"- **{label}** ({min_l}-{max_l}ms): {count}ä¸ª ({pct:.1f}%)")

        lines.extend([
            f"",
            f"---",
            f"",
            f"*æŠ¥å‘Šç”± {PROGRAM_NAME} v{VERSION} ç”Ÿæˆ*",
        ])

        # å†™å…¥æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

        logger.info(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")

    except Exception as e:
        logger.error(f"ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šå¤±è´¥: {e}")

# ==================== HTTP APIæœåŠ¡ ====================

class HostsHTTPHandler(BaseHTTPRequestHandler):
    """HTTPè¯·æ±‚å¤„ç†å™¨"""

    def log_message(self, format, *args):
        """é‡å†™æ—¥å¿—æ–¹æ³•"""
        logger.debug(f"{self.address_string()} - {format % args}")

    def do_GET(self):
        """å¤„ç†GETè¯·æ±‚"""
        parsed_path = urlparse(self.path)
        path = parsed_path.path

        try:
            if path == '/':
                self.send_response(200)
                self.send_header('Content-type', 'text/html; charset=utf-8')
                self.end_headers()
                self.wfile.write(self._generate_index_page().encode())

            elif path == '/hosts':
                self.send_response(200)
                self.send_header('Content-type', 'text/plain; charset=utf-8')
                self.end_headers()
                results = global_state.get_results()
                if results:
                    content = generate_hosts_content(results, 'extended', multi_ip=True)
                    self.wfile.write(content.encode())
                else:
                    self.wfile.write(b"# Hosts file not generated yet")

            elif path == '/stats':
                self.send_response(200)
                self.send_header('Content-type', 'application/json; charset=utf-8')
                self.end_headers()
                stats = global_state.get_stats()
                self.wfile.write(json.dumps(stats, indent=2, ensure_ascii=False).encode())

            elif path == '/health':
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                health = {
                    'status': 'healthy',
                    'version': VERSION,
                    'last_update': global_state.last_update.isoformat() if global_state.last_update else None
                }
                self.wfile.write(json.dumps(health).encode())

            else:
                self.send_response(404)
                self.end_headers()
                self.wfile.write(b"404 Not Found")

        except Exception as e:
            logger.error(f"å¤„ç†è¯·æ±‚å¤±è´¥: {e}")
            self.send_response(500)
            self.end_headers()
            self.wfile.write(f"Internal Server Error: {e}".encode())

    def _generate_index_page(self) -> str:
        """ç”Ÿæˆé¦–é¡µ"""
        stats = global_state.get_stats()
        return f"""<!DOCTYPE html>
<html>
<head>
    <title>{PROGRAM_NAME} v{VERSION}</title>
    <meta charset="utf-8">
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; background: #f5f5f5; }}
        .container {{ max-width: 800px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        h1 {{ color: #333; border-bottom: 2px solid #4CAF50; padding-bottom: 10px; }}
        .stats {{ background: #f9f9f9; padding: 15px; border-radius: 4px; margin: 20px 0; }}
        .endpoint {{ background: #e3f2fd; padding: 10px; margin: 10px 0; border-radius: 4px; }}
        .endpoint code {{ background: #fff; padding: 5px 10px; border-radius: 3px; }}
        a {{ color: #2196F3; text-decoration: none; }}
        a:hover {{ text-decoration: underline; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>{PROGRAM_NAME}</h1>
        <p><strong>ç‰ˆæœ¬</strong>: {VERSION}</p>
        <p><strong>çŠ¶æ€</strong>: ğŸŸ¢ è¿è¡Œä¸­</p>

        <div class="stats">
            <h3>ğŸ“Š ç»Ÿè®¡ä¿¡æ¯</h3>
            <p><strong>æœ€åæ›´æ–°</strong>: {global_state.last_update.strftime('%Y-%m-%d %H:%M:%S') if global_state.last_update else 'æœªç”Ÿæˆ'}</p>
            {f'<p><strong>æˆåŠŸç‡</strong>: {stats["success_count"]}/{stats["total_domains"]} ({stats["success_rate"]:.1f}%)</p>' if stats else ''}
            {f'<p><strong>è€—æ—¶</strong>: {stats["elapsed_time"]:.2f}ç§’</p>' if stats else ''}
        </div>

        <h3>ğŸ”— APIç«¯ç‚¹</h3>
        <div class="endpoint">
            <strong>GET /hosts</strong><br>
            è·å–æœ€æ–°çš„hostsæ–‡ä»¶<br>
            <code>curl http://localhost:{DEFAULT_HTTP_PORT}/hosts</code>
        </div>

        <div class="endpoint">
            <strong>GET /stats</strong><br>
            è·å–ç»Ÿè®¡ä¿¡æ¯ï¼ˆJSONæ ¼å¼ï¼‰<br>
            <code>curl http://localhost:{DEFAULT_HTTP_PORT}/stats</code>
        </div>

        <div class="endpoint">
            <strong>GET /health</strong><br>
            å¥åº·æ£€æŸ¥<br>
            <code>curl http://localhost:{DEFAULT_HTTP_PORT}/health</code>
        </div>

        <h3>ğŸ“– ä½¿ç”¨æ–¹æ³•</h3>
        <pre><code># ä¸‹è½½hostsæ–‡ä»¶
curl http://localhost:{DEFAULT_HTTP_PORT}/hosts >> /etc/hosts

# æŸ¥çœ‹ç»Ÿè®¡
curl http://localhost:{DEFAULT_HTTP_PORT}/stats | jq</code></pre>
    </div>
</body>
</html>"""

def start_http_server(port: int):
    """å¯åŠ¨HTTPæœåŠ¡å™¨"""
    try:
        server = HTTPServer(('0.0.0.0', port), HostsHTTPHandler)
        logger.info(f"ğŸŒ HTTPæœåŠ¡å·²å¯åŠ¨: http://0.0.0.0:{port}")
        logger.info(f"   - è®¿é—® http://localhost:{port}/ æŸ¥çœ‹çŠ¶æ€")
        logger.info(f"   - è®¿é—® http://localhost:{port}/hosts ä¸‹è½½hosts")
        logger.info(f"   - è®¿é—® http://localhost:{port}/stats æŸ¥çœ‹ç»Ÿè®¡")
        server.serve_forever()
    except Exception as e:
        logger.error(f"HTTPæœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")

# ==================== Daemonæ¨¡å¼ ====================

def daemon_worker(args):
    """Daemonå·¥ä½œçº¿ç¨‹"""
    logger.info(f"ğŸ”„ Daemonæ¨¡å¼å¯åŠ¨ï¼Œæ›´æ–°é—´éš”: {args.interval}ç§’")

    while global_state.is_running:
        try:
            logger.info("å¼€å§‹æ›´æ–°hosts...")
            generate_hosts_file(
                args.output,
                args.level,
                not args.no_doh,
                not args.no_cache,
                not args.no_web,
                not args.no_multi_ip
            )

            if args.report:
                results = global_state.get_results()
                stats = global_state.get_stats()
                generate_stats_report(results, stats, 'stats_report.md')

            logger.info(f"âœ… æ›´æ–°å®Œæˆï¼Œä¸‹æ¬¡æ›´æ–°æ—¶é—´: {args.interval}ç§’å")

            # ç­‰å¾…ä¸‹ä¸€æ¬¡æ›´æ–°
            for _ in range(args.interval):
                if not global_state.is_running:
                    break
                time.sleep(1)

        except Exception as e:
            logger.error(f"Daemonæ›´æ–°å¤±è´¥: {e}")
            time.sleep(60)  # å¤±è´¥åç­‰å¾…1åˆ†é’Ÿ

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨"""
    logger.info("\næ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
    global_state.stop()
    sys.exit(0)

# ==================== ä¸»ç¨‹åº ====================

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description=f'{PROGRAM_NAME} v{VERSION}',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # CLIæ¨¡å¼ï¼ˆä¸€æ¬¡æ€§ç”Ÿæˆï¼‰
  python %(prog)s --level=extended

  # Daemonæ¨¡å¼ï¼ˆæŒç»­æœåŠ¡ï¼‰
  python %(prog)s --daemon --interval=600 --port=8080

  # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
  python %(prog)s --level=full --report
        """
    )

    parser.add_argument('--level', choices=['core', 'extended', 'full'], default='extended',
                        help='åŸŸåçº§åˆ« [é»˜è®¤: extended]')
    parser.add_argument('--output', default='github_hosts_ultimate',
                        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ [é»˜è®¤: github_hosts_ultimate]')
    parser.add_argument('--no-doh', action='store_true', help='ç¦ç”¨DoH')
    parser.add_argument('--no-cache', action='store_true', help='ç¦ç”¨ç¼“å­˜')
    parser.add_argument('--no-web', action='store_true', help='ç¦ç”¨Webçˆ¬è™«é™çº§')
    parser.add_argument('--no-multi-ip', action='store_true', help='ç¦ç”¨å¤šIPè½®è¯¢')
    parser.add_argument('--report', action='store_true', help='ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š')

    # Daemonæ¨¡å¼å‚æ•°
    parser.add_argument('--daemon', action='store_true', help='Daemonæ¨¡å¼ï¼ˆæŒç»­è¿è¡Œï¼‰')
    parser.add_argument('--interval', type=int, default=DEFAULT_DAEMON_INTERVAL,
                        help=f'Daemonæ›´æ–°é—´éš”ï¼ˆç§’ï¼‰ [é»˜è®¤: {DEFAULT_DAEMON_INTERVAL}]')
    parser.add_argument('--port', type=int, default=DEFAULT_HTTP_PORT,
                        help=f'HTTPæœåŠ¡ç«¯å£ [é»˜è®¤: {DEFAULT_HTTP_PORT}]')

    parser.add_argument('--version', action='version', version=f'{PROGRAM_NAME} v{VERSION}')

    args = parser.parse_args()

    # æ£€æŸ¥ä¾èµ–
    try:
        import dns.resolver
        import requests
    except ImportError as e:
        logger.error(f"ç¼ºå°‘ä¾èµ–: {e}")
        logger.error("è¯·å®‰è£…: pip install dnspython requests")
        sys.exit(1)

    # æ³¨å†Œä¿¡å·å¤„ç†
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    try:
        if args.daemon:
            # Daemonæ¨¡å¼
            logger.info(f"ğŸš€ å¯åŠ¨ {PROGRAM_NAME} v{VERSION} - Daemonæ¨¡å¼")

            # å¯åŠ¨HTTPæœåŠ¡å™¨çº¿ç¨‹
            http_thread = threading.Thread(target=start_http_server, args=(args.port,), daemon=True)
            http_thread.start()

            # è¿è¡ŒDaemonå·¥ä½œçº¿ç¨‹
            daemon_worker(args)

        else:
            # CLIæ¨¡å¼
            logger.info(f"ğŸš€ å¯åŠ¨ {PROGRAM_NAME} v{VERSION} - CLIæ¨¡å¼")

            results, stats = generate_hosts_file(
                args.output,
                args.level,
                not args.no_doh,
                not args.no_cache,
                not args.no_web,
                not args.no_multi_ip
            )

            if args.report:
                generate_stats_report(results, stats, 'stats_report.md')

    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        logger.error(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == '__main__':
    main()
